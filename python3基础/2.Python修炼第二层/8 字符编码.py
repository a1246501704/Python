https://www.cnblogs.com/linhaifeng/articles/5950339.html

文本编辑器存取文件的原理（nodepad++，pycharm，word）:
# 1、打开编辑器就打开了启动了一个进程，是在内存中的，所以，用编辑器编写的内容也都是存放与内存中的，断电后数据丢失
# 2、要想永久保存，需要点击保存按钮：编辑器把内存的数据刷到了硬盘上。
# 3、在我们编写一个py文件（没有执行），跟编写其他文件没有任何区别，都只是在编写一堆字符而已。

python解释器执行py文件的原理 ，例如python test.py:
# 第一阶段：python解释器启动，此时就相当于启动了一个文本编辑器
# 第二阶段：python解释器相当于文本编辑器，去打开test.py文件，从硬盘上将test.py的文件内容读入到内存中(小复习：
#         pyhon的解释性，决定了解释器只关心文件内容，不关心文件后缀名)
# 第三阶段：python解释器解释执行刚刚加载到内存中test.py的代码( ps：在该阶段，即真正执行代码时，才会识别python的语法，
#         执行文件内代码，当执行到name="egon"时,会开辟内存空间存放字符串"egon")

总结python解释器与文件本编辑的异同:
#  1、相同点：python解释器是解释执行文件内容的，因而python解释器具备读py文件的功能，这一点与文本编辑器一样.
#  2、不同点：文本编辑器将文件内容读入内存后，是为了显示或者编辑，根本不去理会python的语法，而python解释器将文件内容读入内存后，
#           可不是为了给你瞅一眼python代码写的啥，而是为了执行python代码、会识别python语法。

什么是字符编码:
# 　　计算机要想工作必须通电,即用‘电’驱使计算机干活,也就是说‘电’的特性决定了计算机的特性。电的特性即高低电平(人类从逻辑上将二进制数1对应高电平,二进制数0对应低电平)，关于磁盘的磁特性也是同样的道理。结论：计算机只认识数字
# 　　很明显，我们平时在使用计算机时，用的都是人类能读懂的字符（用高级语言编程的结果也无非是在文件内写了一堆字符），如何能让计算机读懂人类的字符？
# 　　必须经过一个过程：
　　# 字符(人)--------（翻译过程）------->数字(计算机) 
　　# 这个过程实际就是一个字符如何对应一个特定数字的标准，这个标准称之为字符编码。

总结字符编码的发展可分为三个阶段(重要):
# 阶段一: 现代计算机起源于美国，最早诞生也是基于英文考虑的ASCII
        ASCII:一个Bytes代表一个字符（英文字符/键盘上的所有其他字符），1Bytes(字节)=8bit(比特位)，8bit可以表示 2**8-1 种变化，即可以表示255个字符
        ASCII最初只用了后七位，127个数字和字符对应关系，已经完全能够代表键盘上所有的字符了（英文字符/键盘的所有其他字符），后来为了将拉丁文也编码进了ASCII表，将最高位255也占用了。
# 阶段二: 为了满足中文和英文，中国人定制了GBK.
       GBK:2Bytes代表一个中文字符(2**16-1=65535)，1Bytes表示一个英文字符(2**8-1=255).
       为了满足其他国家，各个国家纷纷定制了自己的编码，日本把日文编到Shift_JIS里，韩国把韩文编到Euc-kr里。
# 阶段三: 各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。如何解决这个问题呢？？？

！！！！！！！！！！！！非常重要！！！！！！！！！！！！
#说白了乱码问题的本质就是不统一，如果我们能统一全世界，规定全世界只能使用一种文字符号，然后统一使用一种编码，那么乱码问题将不复存在，
#ps：就像当年秦始皇统一中国一样，书同文车同轨，所有的麻烦事全部解决
#很明显，上述的假设是不可能成立的。很多地方或老的系统、应用软件仍会采用各种各样的编码，这是历史遗留问题。于是我们必须找出一种解决方案或者说编码方案，需要同时满足：
# 1、能够兼容万国字符
# 2、与全世界所有的字符编码都有映射关系，这样就可以转换成任意国家的字符编码。

# 统一标准（unicode与各国语言都有映射关系）
这就是unicode（定长），统一用:2Bytes代表一个字符，虽然2**16-1=65535，但unicode却可以存放100w+个字符，因为unicode存放了与其他所有编码的映射关系，
准确地说unicode并不是一种严格意义上的字符编码表，下载pdf来查看unicode的详情：链接：https://pan.baidu.com/s/1dEV3RYp

# 补充标准（UTF-8）
很明显对于通篇都是英文的文本来说，unicode的式无疑是多了一倍的存储空间（二进制最终都是以电或者磁的方式存储到存储介质中的）
于是产生了UTF-8，作为unucode的补充。（可变长，全称Unicode Transformation Format），对英文字符只用:1Bytes表示，对中文字符用:3Bytes，对其他生僻字用更多的Bytes去存。
# 总结：内存中统一采用unicode，浪费空间来换取可以转换成任意编码（不乱码），硬盘可以采用各种编码，如utf-8，保证存放于硬盘或者基于网络传输的数据量很小，提高传输效率与稳定性。
# 为什么没用utf-8做为统一标准？因为最开始还没有utf-8编码


！！！总结非常重要的两点！！！
# 1、保证不乱码的核心法则就是，字符按照什么标准而编码的，就要按照什么标准解码，此处的标准指的就是字符编码。
# 2、在内存中写的所有字符，一视同仁，都是unicode编码，比如我们打开编辑器，输入一个“你”，我们并不能说“你”就是一个汉字，
#    此时它仅仅只是一个符号，该符号可能很多国家都在使用，根据我们使用的输入法不同这个字的样式可能也不太一样。只有在我们
#    往硬盘保存或者基于网络传输时，才能确定”你“到底是一个汉字，还是一个日本字，这就是unicode转换成其他编码格式的过程了。

unicode----->encode-------->utf-8(或者其他编码)

utf-8(或者其他编码)-------->decode---------->unicode(python的str类型)

# 存取阶段
编辑器（日文编码）——>同时写入中文、日文到内存中（此时是unicode，都不会乱码）——>点击保存存入硬盘（存入时用编辑器端的日文编码存，此时编辑器上看着好像都是正常的）——>关闭文件重新打开，就是从硬盘安读取（中文乱码了，日文正常）
编辑器（gbk）——>写入中文“你好”保存——>打开cmd执行python3 文件名——>打开文件（文件头用conding：utf-8 的作用就是告诉编辑器用什么编码打开）——>执行正常

# 执行阶段
'''
python2
#coding:gbk
x='上'.decode('gbk')
y='下'.decode('gbk')
print([x,y]) # [u'\u4e0a', u'\u4e0b']

在python3 中也有两种字符串类型str和bytes,str是unicode。
#coding:gbk
x='上'                         # 当程序执行时，无需加u，'上'也会被以unicode形式保存新的内存空间中。
print(type(x))                 # <class 'str'>，x可以直接encode成任意编码格式。
print(x.encode('gbk'))         # b'\xc9\xcf'
print(type(x.encode('gbk')))   #<class 'bytes'>
'''
# 编码: encode
unicode ——> encode ——> 其他编码（encode后的类型是bytes类型）
# 解码: decode
其他编码 ——> decode（以什么编码存就以什么编码解） ——> unicode


从磁盘读：磁盘(存的时候用的是某个国家编码标准)=======> unicode(加载到内存中) =====> encode(转换，文件头) ======>编辑器（gbk、utf8....全世界各国编码(输出)）
往磁盘写：编辑器用gbk、utf-8...全世界各国编码(输入)  ====> unicode(内存中写什么都不乱码) ======> decode(点击保存) =====> 磁盘(对应写时使用的编码)


\博客讲解
# python2中
字符串编码常用类型：utf-8,gb2312,cp936,gbk等。python中，我们使用decode()和encode()来进行解码和编码
在python中，使用unicode类型作为编码的基础类型。即

     decode              encode
str ---------> unicode --------->str

u = u'中文'               # 显示指定unicode类型对象u。
str = u.encode('gb2312') # 以gb2312编码对unicode对像进行编码。
str1 = u.encode('gbk')   # 以gbk编码对unicode对像进行编码。
str2 = u.encode('utf-8') # 以utf-8编码对unicode对像进行编码。
u1 = str.decode('gb2312')# 以gb2312编码对字符串str进行解码，以获取unicode。
u2 = str.decode('utf-8') # 如果以utf-8的编码对str进行解码得到的结果，将无法还原原来的unicode类型。

# python3中
如上面代码，str、str1、str2均为字符串类型（str）,给字符串操作带来较大的复杂性。
好消息来了，对，那就是python3，在新版本的python3中，取消了unicode类型，代替它的是使用unicode字符的字符串类型(str),字符串类型（str）成为中间基础类型。
如下所示，而编码后的变为了字节类型(bytes)但是两个函数的使用方法不变：

      decode              encode
bytes ------> str(unicode)------>bytes

u = '中文'                 # 指定字符串类型对象u。
str = u.encode('gb2312')  # 以gb2312编码对u进行编码，获得bytes类型对象str。
u1  = str.decode('gb2312')# 以gb2312编码对字符串str进行解码，获得字符串类型对象u1。
u2  = str.decode('utf-8') # 如果以utf-8的编码对str进行解码得到的结果，将无法还原原来的字符串内容。用什么类型的编码的就要用什么解码。

a='张洪洋'
b= a.encode('utf-8')
print(b)
c = b.decode('utf-8')
print(c)
# b'\xe5\xbc\xa0\xe6\xb4\xaa\xe6\xb4\x8b'
# 张洪洋

避免不了的是，文件读取问题：
假如我们读取一个文件，文件保存时，使用的编码格式，决定了我们从文件读取的内容的编码格式，例如，我们从记事本新建一个文本文件test.txt, 
编辑内容，保存的时候注意，编码格式是可以选择的，例如我们可以选择gb2312,那么使用python读取文件内容，方式如下：

f = open('test.txt','r') # 文件内容写入时使用的是gb2312编码
s = f.read()             # 读取文件内容,如果是不识别的encoding格式（识别的encoding类型跟使用的系统有关），这里将读取失败
'''假设文件保存时以gb2312编码保存'''
u = s.decode('gb2312')   # 以文件保存格式对内容进行解码，获得unicode字符串
'''下面我们就可以对内容进行各种编码的转换了'''
str =  u.encode('utf-8') # 转换为utf-8编码的字符串str
str1 = u.encode('gbk')   # 转换为gbk编码的字符串str1
str1 = u.encode('utf-16')# 转换为utf-16编码的字符串str1

python给我们提供了一个包codecs进行文件的读取，这个包中的open()函数可以指定编码的类型：
import codecs
f = codecs.open('text.text','r+',encoding='utf-8') # 必须事先知道文件的编码格式，这里文件编码是使用的utf-8
content = f.read()      # 如果open时使用的encoding和文件本身的encoding不一致的话，那么这里将将会产生错误
f.write('你想要写入的信息')
f.close()